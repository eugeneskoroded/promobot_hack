{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504c8910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 05:12:23.067389: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-09-21 05:12:23.067425: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from typing import List\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86da5a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/netcrk/cp39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/netcrk/cp39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/netcrk/cp39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006437063217163086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 23,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d874d50f5c4141f992d3ae945954a202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_to_model = \"../Llama-2-13b-chat-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(path_to_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    path_to_model, \n",
    "    load_in_8bit=True, \n",
    "    device_map='auto', \n",
    "    torch_dtype=torch.float16\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c46efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a617ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_prompt(schema: str, context: str, n_examples: int) -> str:\n",
    "    return f\"\"\"[INST] <<SYS>>\n",
    "                You are QA Engineer and you need to generate {n_examples} test examples with following schema to test API: {schema} \n",
    "                Generate your answer in following format: \"examples\": []. example word has to be in double qoutes.\n",
    "                Take this context into account when generating: {context}. Answer without any description.\n",
    "                <</SYS>>[/INST]\"\"\"\n",
    "\n",
    "\n",
    "def generate_test_examples(inputs: List[dict], \n",
    "                           n_examples: int = 5, \n",
    "                           batch_size: int = 10,\n",
    "                           max_new_tokens: int = 300) -> str: \n",
    "    \n",
    "    input_prompts = [get_input_prompt(input_[\"schema\"], input_[\"context\"], n_examples) for input_ in inputs]\n",
    "    \n",
    "    dataloader = DataLoader(input_prompts, batch_size=batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_outputs = []\n",
    "        \n",
    "        for batch in tqdm(dataloader):\n",
    "            model_input = tokenizer(batch, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        \n",
    "            model_output = model.generate(**model_input, max_new_tokens=max_new_tokens)\n",
    "            model_output = tokenizer.batch_decode(model_output, skip_special_tokens=True)\n",
    "            \n",
    "            model_outputs.extend(model_output)\n",
    "        \n",
    "    return model_outputs\n",
    "\n",
    "\n",
    "def clear_response(responses: List[str]) -> str:\n",
    "    outputs = []\n",
    "    \n",
    "    for response in responses:\n",
    "        try:\n",
    "            response = response.split(\"\\n\\n\")[1]\n",
    "            response = json.loads(\"{\" + response + \"}\")\n",
    "            \n",
    "            outputs.append(response)\n",
    "        except:\n",
    "            outputs.append({\"status\": \"Error while parsing.\", \"response\": response})\n",
    "            \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7253e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5016\n",
      " * Running on http://10.112.2.242:5016\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007230997085571289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 23,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9548fab73e3b4673a70b36ad28a2f0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10.236.151.95 - - [21/Sep/2023 05:13:40] \"GET /?n_examples=3&batch_size=10&max_new_tokens=300 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00648188591003418,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 23,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70264a8f403463fb1178e938225a6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10.236.151.95 - - [21/Sep/2023 05:14:08] \"GET /?n_examples=3&batch_size=10&max_new_tokens=300 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006754159927368164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 23,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214e08b6f23b414a908e3ca4d6d8a64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10.236.151.95 - - [21/Sep/2023 05:16:26] \"GET /?n_examples=3&batch_size=10&max_new_tokens=300 HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\"])\n",
    "def get_test_examples():\n",
    "    inputs, params = request.json[\"inputs\"], request.args\n",
    "    \n",
    "    n_examples = params.get(\"n_examples\") or 3\n",
    "    batch_size = params.get(\"batch_size\") or 5\n",
    "    max_new_tokens = params.get(\"max_new_tokens\") or 300\n",
    "    \n",
    "    response = generate_test_examples(inputs, n_examples, int(batch_size), int(max_new_tokens))\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    return clear_response(response)\n",
    "\n",
    "app.run(host=\"0.0.0.0\", port=5016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d559b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
