{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "963d5ac1-1228-47a8-835b-f0ba34b3908c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap-learn in /opt/conda/lib/python3.10/site-packages (0.5.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (1.3.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (0.58.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (0.5.11)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from umap-learn) (4.65.0)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe1dee8-2f98-4f68-adaf-c3f28b9fe1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:149: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_config.py:321: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "import torch\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cfc34a03-f05c-4dfb-a2b2-4283e1acd28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "import pandas as pd\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords');\n",
    "stopwords = set(stopwords.words('russian'))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78ae2e49-3e29-48ec-9474-bc60c48bc494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    words = text.split() # разбиваем текст на слова\n",
    "    #print(words)\n",
    "    res = list()\n",
    "    for word in words:\n",
    "        if (word not in stopwords and len(word) > 1):\n",
    "            p = morph.parse(word)[0]\n",
    "            res.append(p.normal_form)\n",
    "    text = \" \".join(res)\n",
    "    return text\n",
    "\n",
    "def text_cleaner(input_text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', input_text)\n",
    "    rem_url = re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_url= re.sub(r'([a-z]{2}\\d+[a-z]{2})',' ',rem_url)\n",
    "    rem_url = re.sub(r'!+','.', rem_url)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    text = re.sub(r\"[-—()\\\"#/@;:<>{}=~|€«»$\\+'_–\\*°“”\\\\√&×•ó÷≈„()‽\\+.,!]+\", \" \", rem_num)\n",
    "    text = re.sub(\"!\",\".\", text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r\"\\[.+\\]\",'', text)\n",
    "    emoji_pattern=re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    text = emoji_pattern.sub('', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def dataset_preprocessor_vectorizers(data, w_count_to_drop=3):\n",
    "    data_frame = data.copy()\n",
    "\n",
    "    indexes_to_drop = []\n",
    "    # Дропаем дубли\n",
    "    data_frame = data_frame.drop_duplicates(subset=['Текст инцидента']).reset_index(drop=True)\n",
    "\n",
    "    # Regex для групп\n",
    "    p = re.compile(\"\\[.+\\|.+\\]\")\n",
    "\n",
    "    for index, row in tqdm(data_frame.iterrows()):\n",
    "\n",
    "        # Очистка текста\n",
    "        data_frame.loc[index, \"Текст инцидента\"] = text_cleaner(data_frame.loc[index, \"Текст инцидента\"])\n",
    "\n",
    "        # Отсекаем сообщения из w_count_to_drop слов\n",
    "        if len(row[\"Текст инцидента\"].split(\" \")) < w_count_to_drop and len(row[\"Текст инцидента\"].split(\".\")) < w_count_to_drop:\n",
    "            indexes_to_drop.append(index)\n",
    "\n",
    "    # Дропаем индексы < w_count_to_drop слов\n",
    "    #print(indexes_to_drop)\n",
    "    data_frame = data_frame.drop(index=indexes_to_drop).reset_index(drop=True)\n",
    "\n",
    "    # Лемматизация и удаление стоп слов\n",
    "    for index, row in tqdm(data_frame.iterrows()):\n",
    "        data_frame.loc[index, \"Текст инцидента\"] = lemmatize(row[\"Текст инцидента\"])\n",
    "    return data_frame\n",
    "train = dataset_preprocessor_vectorizers(train)\n",
    "test = dataset_preprocessor_vectorizers(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47788226-70ae-4067-ad8b-f1807c92e7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7761b869-0a84-4fdc-aeb4-9e3cec2982ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"datasets/sbert_large_mt_nlu_ru_train_mean.csv\")\n",
    "test = pd.read_csv(\"datasets/sbert_large_mt_nlu_ru_test_mean.csv\")\n",
    "embeddings_columns = [str(i) for i in range(1024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4633ee-dd10-42d1-ade6-bee8f165a758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "adb33f1d-b325-4dde-8f7b-f40702d7e76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e82c8720a174ad7a884b5598ddfad47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0186f78a6a104465b253e5afbcb1ce95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c7f9109b604307ae17be204e77de39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186c28b932c54edeb2aa889e452f062f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = dataset_preprocessor_vectorizers(train)\n",
    "test = dataset_preprocessor_vectorizers(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1a888e9-45c8-400c-8267-d7b288fe6542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Текст инцидента</th>\n",
       "      <th>Группа тем</th>\n",
       "      <th>Исполнитель</th>\n",
       "      <th>Тема</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>краснокамск новостикраснокамск объявлениякрасн...</td>\n",
       "      <td>Мусор/Свалки/ТКО</td>\n",
       "      <td>АО ПРО ТКО</td>\n",
       "      <td>★ Уборка/Вывоз мусора</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ещё два соцзащита продублировать деньга выплат...</td>\n",
       "      <td>Социальное обслуживание и защита</td>\n",
       "      <td>Министерство социального развития ПК</td>\n",
       "      <td>Дети и многодетные семьи</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>добрый день коминтерн комната иметься документ...</td>\n",
       "      <td>ЖКХ</td>\n",
       "      <td>ИГЖН ПК</td>\n",
       "      <td>Жалобы на управляющие компании</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>дом сосулька пермский край</td>\n",
       "      <td>Социальное обслуживание и защита</td>\n",
       "      <td>Город Пермь</td>\n",
       "      <td>Аварийное жилье/переселение</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>пермь просить позортесь отремонтировать остано...</td>\n",
       "      <td>Общественный транспорт</td>\n",
       "      <td>Город Пермь</td>\n",
       "      <td>Содержание остановок</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15087</th>\n",
       "      <td>класс вопрос построить спортивный комплекс рай...</td>\n",
       "      <td>Физическая культура и спорт</td>\n",
       "      <td>Город Пермь</td>\n",
       "      <td>Строительство спортивной инфраструктуры</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15088</th>\n",
       "      <td>здравствуйте кунгур привиться завтра?</td>\n",
       "      <td>Коронавирус</td>\n",
       "      <td>Министерство здравоохранения</td>\n",
       "      <td>Порядок и пункты вакцинации</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15089</th>\n",
       "      <td>женщина сделать кесарев пермь день выписать до...</td>\n",
       "      <td>Здравоохранение/Медицина</td>\n",
       "      <td>Министерство здравоохранения</td>\n",
       "      <td>★ Оказание медицинской помощи не в полном объе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15090</th>\n",
       "      <td>вопросовчиновникамный набережная мотовилихинск...</td>\n",
       "      <td>Дороги</td>\n",
       "      <td>Город Пермь</td>\n",
       "      <td>Необходима установка и замена дорожных ограждений</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15091</th>\n",
       "      <td>ким половина год горячий вода олег валентинови...</td>\n",
       "      <td>ЖКХ</td>\n",
       "      <td>Александровский муниципальный округ Пермского ...</td>\n",
       "      <td>Отсутствие горячей воды</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15092 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Текст инцидента  \\\n",
       "0      краснокамск новостикраснокамск объявлениякрасн...   \n",
       "1      ещё два соцзащита продублировать деньга выплат...   \n",
       "2      добрый день коминтерн комната иметься документ...   \n",
       "3                             дом сосулька пермский край   \n",
       "4      пермь просить позортесь отремонтировать остано...   \n",
       "...                                                  ...   \n",
       "15087  класс вопрос построить спортивный комплекс рай...   \n",
       "15088              здравствуйте кунгур привиться завтра?   \n",
       "15089  женщина сделать кесарев пермь день выписать до...   \n",
       "15090  вопросовчиновникамный набережная мотовилихинск...   \n",
       "15091  ким половина год горячий вода олег валентинови...   \n",
       "\n",
       "                             Группа тем  \\\n",
       "0                      Мусор/Свалки/ТКО   \n",
       "1      Социальное обслуживание и защита   \n",
       "2                                   ЖКХ   \n",
       "3      Социальное обслуживание и защита   \n",
       "4                Общественный транспорт   \n",
       "...                                 ...   \n",
       "15087       Физическая культура и спорт   \n",
       "15088                       Коронавирус   \n",
       "15089          Здравоохранение/Медицина   \n",
       "15090                            Дороги   \n",
       "15091                               ЖКХ   \n",
       "\n",
       "                                             Исполнитель  \\\n",
       "0                                             АО ПРО ТКО   \n",
       "1                   Министерство социального развития ПК   \n",
       "2                                                ИГЖН ПК   \n",
       "3                                            Город Пермь   \n",
       "4                                            Город Пермь   \n",
       "...                                                  ...   \n",
       "15087                                        Город Пермь   \n",
       "15088                       Министерство здравоохранения   \n",
       "15089                       Министерство здравоохранения   \n",
       "15090                                        Город Пермь   \n",
       "15091  Александровский муниципальный округ Пермского ...   \n",
       "\n",
       "                                                    Тема  \n",
       "0                                  ★ Уборка/Вывоз мусора  \n",
       "1                               Дети и многодетные семьи  \n",
       "2                         Жалобы на управляющие компании  \n",
       "3                            Аварийное жилье/переселение  \n",
       "4                                   Содержание остановок  \n",
       "...                                                  ...  \n",
       "15087            Строительство спортивной инфраструктуры  \n",
       "15088                        Порядок и пункты вакцинации  \n",
       "15089  ★ Оказание медицинской помощи не в полном объе...  \n",
       "15090  Необходима установка и замена дорожных ограждений  \n",
       "15091                            Отсутствие горячей воды  \n",
       "\n",
       "[15092 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[[\"Текст инцидента\", \"Группа тем\", \"Исполнитель\", \"Тема\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "865b7e30-388a-4544-bae2-e02ac6a43e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class Model_tfidf_emb_umap:\n",
    "    def __init__(self, vectorizer_name, vectorizer, max_features=None, emb_length=1024, emb_umap_size=100, vectorizer_umap_size=500):\n",
    "        self.max_features = max_features\n",
    "        self.vectorizer = vectorizer(max_features=self.max_features)\n",
    "        self.vectorizer_name = vectorizer_name\n",
    "        self.emb_length = emb_length\n",
    "    \n",
    "    def create_umap(self, emb_names, train, test, cols_for_umap, n_components=100):\n",
    "        column_names = [emb_names+f\"_{i}\" for i in range(n_components)]\n",
    "        ump = umap.UMAP(n_components=n_components, random_state=42)\n",
    "        train_data_frame = train.copy()\n",
    "        test_data_frame = test.copy()\n",
    "        train_emb = ump.fit_transform(train_data_frame[cols_for_umap])\n",
    "        test_emb = ump.transform(test_data_frame[cols_for_umap])\n",
    "        \n",
    "        return pd.DataFrame(train_emb, columns=column_names) , pd.DataFrame(test_emb, columns=column_names)\n",
    "    \n",
    "    def create_tfidf(self, train, test):\n",
    "      tf = self.vectorizer.fit_transform(train)\n",
    "      with open(f'vectorizers/{self.vectorizer_name}_vectorizer_{self.max_features}.pk', 'wb') as fin:\n",
    "        pickle.dump(self.vectorizer, fin)\n",
    "      vocab = self.vectorizer.vocabulary_\n",
    "      train_tf, test_tf = self.create_umap(\"tfidf\", pd.DataFrame(tf.toarray(), columns=sorted(vocab.keys())),\n",
    "                             pd.DataFrame(self.vectorizer.transform(test).toarray(), columns=sorted(vocab.keys())),\n",
    "                                           cols_for_umap=sorted(vocab.keys()), \n",
    "                                           n_components=350)\n",
    "      del tf\n",
    "      return train_tf, test_tf\n",
    "\n",
    "\n",
    "    def create_datasets(self, train, test):\n",
    "        train_, test_ = train.copy(), test.copy()\n",
    "        \n",
    "        embeddings_columns = [str(i) for i in range(self.emb_length)]\n",
    "        train_emb, test_emb = self.create_umap(\"bert_emb\", \n",
    "                                          train_[embeddings_columns] ,\n",
    "                                          test_[embeddings_columns], \n",
    "                                          embeddings_columns, 100)\n",
    "        train_ = train_[[\"Текст инцидента\", \"Группа тем\", \"Исполнитель\", \"Тема\"]]\n",
    "        test_ = test_[[\"Текст инцидента\", \"Группа тем\", \"Исполнитель\", \"Тема\"]]\n",
    "        \n",
    "        train_ = pd.concat([train_, train_emb], axis=1)\n",
    "        test_ = pd.concat([test_, test_emb], axis=1)\n",
    "        \n",
    "        train_tf_idf, test_tf_idf = self.create_tfidf(train_[\"Текст инцидента\"].to_list(), test_[\"Текст инцидента\"].to_list())\n",
    "        \n",
    "       \n",
    "        \n",
    "        train_ = pd.concat([train_, train_tf_idf], axis=1)\n",
    "        test_ = pd.concat([test_, test_tf_idf], axis=1)\n",
    "        \n",
    "        del train_tf_idf\n",
    "        del test_tf_idf\n",
    "        #train_.to_csv(f\"datasets/{self.vectorizer_name}_train_{self.max_features}.csv\", index=False)\n",
    "        #test_.to_csv(f\"datasets/{self.vectorizer_name}_test_{self.max_features}.csv\", index=False)\n",
    "\n",
    "        return train_, test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ea340ed3-9f12-4cb9-a3b2-896652fcd717",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mtf \u001b[38;5;241m=\u001b[39m Model_tfidf_emb_umap(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m\"\u001b[39m, TfidfVectorizer, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, emb_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m) \n\u001b[0;32m----> 2\u001b[0m train, test \u001b[38;5;241m=\u001b[39m \u001b[43mmtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[96], line 36\u001b[0m, in \u001b[0;36mModel_tfidf_emb_umap.create_datasets\u001b[0;34m(self, train, test)\u001b[0m\n\u001b[1;32m     33\u001b[0m train_, test_ \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mcopy(), test\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     35\u001b[0m embeddings_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_length)]\n\u001b[0;32m---> 36\u001b[0m train_emb, test_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_umap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert_emb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtrain_\u001b[49m\u001b[43m[\u001b[49m\u001b[43membeddings_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtest_\u001b[49m\u001b[43m[\u001b[49m\u001b[43membeddings_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43membeddings_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m train_ \u001b[38;5;241m=\u001b[39m train_[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mТекст инцидента\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mГруппа тем\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mИсполнитель\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mТема\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     41\u001b[0m test_ \u001b[38;5;241m=\u001b[39m test_[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mТекст инцидента\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mГруппа тем\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mИсполнитель\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mТема\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[96], line 14\u001b[0m, in \u001b[0;36mModel_tfidf_emb_umap.create_umap\u001b[0;34m(self, emb_names, train, test, cols_for_umap, n_components)\u001b[0m\n\u001b[1;32m     12\u001b[0m train_data_frame \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     13\u001b[0m test_data_frame \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 14\u001b[0m train_emb \u001b[38;5;241m=\u001b[39m \u001b[43mump\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_frame\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcols_for_umap\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m test_emb \u001b[38;5;241m=\u001b[39m ump\u001b[38;5;241m.\u001b[39mtransform(test_data_frame[cols_for_umap])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(train_emb, columns\u001b[38;5;241m=\u001b[39mcolumn_names) , pd\u001b[38;5;241m.\u001b[39mDataFrame(test_emb, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/umap/umap_.py:2887\u001b[0m, in \u001b[0;36mUMAP.fit_transform\u001b[0;34m(self, X, y, force_all_finite)\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   2852\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed\u001b[39;00m\n\u001b[1;32m   2853\u001b[0m \u001b[38;5;124;03m    output.\u001b[39;00m\n\u001b[1;32m   2854\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2885\u001b[0m \u001b[38;5;124;03m        Local radii of data points in the embedding (log-transformed).\u001b[39;00m\n\u001b[1;32m   2886\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2887\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2889\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dens:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/umap/umap_.py:2780\u001b[0m, in \u001b[0;36mUMAP.fit\u001b[0;34m(self, X, y, force_all_finite)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2777\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2778\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs\n\u001b[1;32m   2779\u001b[0m     )\n\u001b[0;32m-> 2780\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_, aux_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_embed_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2781\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# JH why raw data?\u001b[39;49;00m\n\u001b[1;32m   2785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2788\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_list\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m aux_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/umap/umap_.py:2826\u001b[0m, in \u001b[0;36mUMAP._fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m   2822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_embed_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, n_epochs, init, random_state):\n\u001b[1;32m   2823\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A method wrapper for simplicial_set_embedding that can be\u001b[39;00m\n\u001b[1;32m   2824\u001b[0m \u001b[38;5;124;03m    replaced by subclasses.\u001b[39;00m\n\u001b[1;32m   2825\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimplicial_set_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2829\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2832\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepulsion_strength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative_sample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2836\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2838\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2839\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2840\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdensmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2841\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_densmap_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2842\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2843\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2844\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2845\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_metric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2846\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2847\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/umap/umap_.py:1192\u001b[0m, in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[1;32m   1185\u001b[0m embedding \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;241m10.0\u001b[39m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;241m*\u001b[39m (embedding \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(embedding, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mmax(embedding, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(embedding, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m   1189\u001b[0m )\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m euclidean_output:\n\u001b[0;32m-> 1192\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_layout_euclidean\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtail\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_vertices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs_per_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m        \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrng_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_sample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdensmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdensmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdensmap_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdensmap_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmove_other\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1214\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m optimize_layout_generic(\n\u001b[1;32m   1215\u001b[0m         embedding,\n\u001b[1;32m   1216\u001b[0m         embedding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1232\u001b[0m         move_other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1233\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/umap/layouts.py:380\u001b[0m, in \u001b[0;36moptimize_layout_euclidean\u001b[0;34m(head_embedding, tail_embedding, head, tail, n_epochs, n_vertices, epochs_per_sample, a, b, rng_state, gamma, initial_alpha, negative_sample_rate, parallel, verbose, densmap, densmap_kwds, tqdm_kwds, move_other)\u001b[0m\n\u001b[1;32m    377\u001b[0m     dens_re_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    378\u001b[0m     dens_re_cov \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 380\u001b[0m \u001b[43moptimize_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtail_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtail\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_vertices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs_per_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmove_other\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs_per_negative_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch_of_next_negative_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch_of_next_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdensmap_flag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdens_phi_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdens_re_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdens_re_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdens_re_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdens_re_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdens_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdens_R\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdens_mu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdens_mu_tot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m alpha \u001b[38;5;241m=\u001b[39m initial_alpha \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m (\u001b[38;5;28mfloat\u001b[39m(n) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(n_epochs)))\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mand\u001b[39;00m n \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(n_epochs \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mtf = Model_tfidf_emb_umap(\"tfidf\", TfidfVectorizer, max_features=5, emb_length=1024) \n",
    "train, test = mtf.create_datasets(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2b101-81e8-4e39-a4d7-fa350e4fab37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "87ff4991-5df8-437d-8857-2b4fd3c4c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(f\"datasets/sbert100_tfidf350_umap_train.csv\", index=False)\n",
    "test.to_csv(f\"datasets/sbert100_tfidf350_umap_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cf236792-cf3e-4a8f-b7aa-83a66f9016f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/24 21:04:50 INFO mlflow.tracking.fluent: Experiment with name 'sbert100_tfidf350_umap_class_weight' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///workspace/mlruns/470649283789315185', creation_time=1700859890086, experiment_id='470649283789315185', last_update_time=1700859890086, lifecycle_stage='active', name='sbert100_tfidf350_umap_class_weight', tags={}>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('sbert100_tfidf350_umap_class_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f6d02e4e-6d41-40c0-8fa2-c23a6769eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'iterations': [100, 200, 400, 500]\n",
    "}\n",
    "grid = list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8b08f062-0893-4371-bd83-b36bb73b138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model_ in models:\n",
    "    #model_name, vectorizer, max_features = model_\n",
    "    #model = Model_tfidf(model_name, vectorizer, max_features)\n",
    "\n",
    "train_, test_ = train, test\n",
    "\n",
    "for params in grid:\n",
    "    try:\n",
    "        with mlflow.start_run(nested=True):\n",
    "            catboost_model = CatBoostClassifier(**params, loss_function='MultiClassOneVsAll', class_weights=class_weights, verbose=0, random_seed=42)\n",
    "            catboost_model.fit(train_.drop(columns=[\"Текст инцидента\", \"Группа тем\", \"Исполнитель\", \"Тема\"]), train_[\"Группа тем\"])\n",
    "\n",
    "            predictions = catboost_model.predict(test_.drop(columns=[\"Текст инцидента\", \"Группа тем\", \"Исполнитель\", \"Тема\"]))\n",
    "\n",
    "            accuracy = accuracy_score(test_[\"Группа тем\"], predictions)\n",
    "            f1 = f1_score(test_[\"Группа тем\"], predictions, average='weighted')\n",
    "            report = classification_report(test_[\"Группа тем\"], predictions, output_dict=True)\n",
    "            report_text =  classification_report(test_[\"Группа тем\"], predictions)\n",
    "\n",
    "\n",
    "            mlflow.log_metric(\"report_accuracy\", report['accuracy'])\n",
    "            mlflow.log_metric(\"macro avg_precision\", report['macro avg']['precision'])\n",
    "            mlflow.log_metric(\"macro avg_recall\", report['macro avg']['recall'])\n",
    "            mlflow.log_metric(\"macro avg_f1-score\", report['macro avg']['f1-score'])\n",
    "            mlflow.log_metric(\"weighted avg_precision\", report['weighted avg']['precision'])\n",
    "            mlflow.log_metric(\"weighted avg_recall\", report['weighted avg']['recall'])\n",
    "            mlflow.log_metric(\"weighted avg_f1-score\", report['weighted avg']['f1-score'])\n",
    "\n",
    "            mlflow.log_text(report_text, \"classification_report.txt\")\n",
    "\n",
    "            \n",
    "            mlflow.set_tag(\"max_features\", 15000)\n",
    "            mlflow.set_tag(\"dataset_name\", \"sbert100_tfidf350_umap\")\n",
    "            mlflow.set_tag(\"model_name\", \"catboost\")\n",
    "            mlflow.catboost.log_model(catboost_model, \"model\")\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_metrics({'accuracy': accuracy, 'f1-weighted': f1})\n",
    "    except Exception as e:\n",
    "        error_name = type(e).name\n",
    "        print(f\"Caught an error: {error_name}\")\n",
    "        print(f\"EXCEPTION: {e}\")\n",
    "        print(sys.exc_info()[0])\n",
    "        traceback.print_exc(file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb42b21-94a3-4bc0-9758-444d3322cdb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
